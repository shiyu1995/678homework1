---
title: "Homework 02"
author: "SHIYU ZHANG"
date: "Septemeber 16, 2018"
output:
  pdf_document: default
---

\newcommand{\mat}[1]{\boldsymbol{#1}} 
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\rv}[1]{\underline{#1}}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,dev="CairoPNG",fig.align = "center", 
                      fig.width = 5.656, fig.height = 4, global.par = TRUE)
pacman::p_load("arm","data.table","Cairo","faraway","foreign","ggplot2","knitr")
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
```

# Introduction 
In homework 2 you will fit many regression models.  You are welcome to explore beyond what the question is asking you.  

Please come see us we are here to help.

## Data analysis 

### Analysis of earnings and height data

The folder `earnings` has data from the Work, Family, and Well-Being Survey (Ross, 1990).
You can find the codebook at http://www.stat.columbia.edu/~gelman/arm/examples/earnings/wfwcodebook.txt
```{r}
gelman_dir <- "http://www.stat.columbia.edu/~gelman/arm/examples/"
heights    <- read.dta (paste0(gelman_dir,"earnings/heights.dta"))
```

Pull out the data on earnings, sex, height, and weight.

1. In R, check the dataset and clean any unusually coded data.

```{r}
library(foreign)
library(arm)
library(ggplot2)

data.new <- read.dta("http://www.stat.columbia.edu/~gelman/arm/examples/earnings/heights.dta")

# remove all the N/A
data.new <- data.new[complete.cases(data.new), ]

# label the sex variable (1 = male, 2 = female)
data.new$sex <- factor(data.new$sex, labels=c("male", "female"))

# remove observations where yearbn > 90
data.new <- data.new[data.new$yearbn <= 90,]

# change the scale of earnings to make the data more readable 
data.new$earn <- data.new$earn / 1000

summary(data.new)


```

2. Fit a linear regression model predicting earnings from height. What transformation should you perform in order to interpret the intercept from this model
as average earnings for people with average height?

```{r}

# normalise `height` and `earn`
data.new$height <- (data.new$height - mean(data.new$height)) / (2 * sd(data.new$height))

model1<- lm(earn ~ height, data=data.new)

model1

display(model1)

```

3. Fit some regression models with the goal of predicting earnings from some
combination of sex, height, and weight. Be sure to try various transformations and interactions that might make sense. Choose your preferred model and justify.

```{r}
model2 <- lm(earn ~ sex * ed + height + yearbn, data=data.new)

display(model2)
```

4. Interpret all model coefficients.

```{r}
"Intercept: the intercept represent the average salary for a male of average age 
and height which has no education 

Sex: female who didn't earn any degree and have average age and height, earn $1,890 
(becasue i used the scale of 1000 in previous question) more than males 
with similar characteristic. 

Education: better education rates corresponds to higher earnings. 

Sex : Education: women's average salary is $9,900 less than what a male 
individual would have"
```

5. Construct 95% confidence interval for all model coefficients and discuss what they mean.

```{r}
confint(model2,level=0.95)

"it means that we are 95% confident that the intercept lies between 6.577 and 6.964
we are 95% confident that the coefficient of ratio lies between -0.005 and 0.0013
we are 95% confident that the coefficient of log(salary) lies between 0.0478 and 0.1682
we are 95% confident that the coefficient of sat taker lies between -0.0936 and -0.0735"

```


### Analysis of mortality rates and various environmental factors

The folder `pollution` contains mortality rates and various environmental factors from 60 U.S. metropolitan areas from McDonald, G.C. and Schwing, R.C. (1973) 'Instabilities of regression estimates relating air pollution to mortality', Technometrics, vol.15, 463-482. 

Variables, in order:

* PREC   Average annual precipitation in inches
* JANT   Average January temperature in degrees F
* JULT   Same for July
* OVR65  % of 1960 SMSA population aged 65 or older
* POPN   Average household size
* EDUC   Median school years completed by those over 22
* HOUS   % of housing units which are sound & with all facilities
* DENS   Population per sq. mile in urbanized areas, 1960
* NONW   % non-white population in urbanized areas, 1960
* WWDRK  % employed in white collar occupations
* POOR   % of families with income < $3000
* HC     Relative hydrocarbon pollution potential
* NOX    Same for nitric oxides
* SO@    Same for sulphur dioxide
* HUMID  Annual average % relative humidity at 1pm
* MORT   Total age-adjusted mortality rate per 100,000

For this exercise we shall model mortality rate given nitric oxides, sulfur dioxide, and hydrocarbons as inputs. This model is an extreme oversimplification as it combines all sources of mortality and does not adjust for crucial factors such as age and smoking. We use it to illustrate log transformations in regression.

```{r}
gelman_dir   <- "http://www.stat.columbia.edu/~gelman/arm/examples/"
pollution    <- read.dta (paste0(gelman_dir,"pollution/pollution.dta"))
```

1. Create a scatterplot of mortality rate versus level of nitric oxides. Do you think linear regression will fit these data well? Fit the regression and evaluate a residual plot from the regression.

```{r}
ggplot(data=pollution)+geom_point(aes(x=nox,y=mort))
# we can see outliners from the graph

pollution$mort <- pollution$mort / 100000

a1<-lm(mort~nox,data=pollution)

a1

par(mfrow=c(2,2))
plot(a1)

plot(y=pollution$mort,x=pollution$nox)
abline(a1)

```

2. Find an appropriate transformation that will result in data more appropriate for linear regression. Fit a regression to the transformed data and evaluate the new residual plot.

```{r}

# use log to improve the model 
a2 <- lm(log(mort) ~ log(nox), data=pollution)

display(a2)

ggplot(data=pollution, aes(x=log(nox), y=log(mort))) + geom_point() + 
  stat_smooth(method="lm", formula=y ~ x, se=TRUE)

# from the new plot ouput, the residuals are evenly distributed around the line. 

par(mfrow=c(2,2))
plot(a2)
```

3. Interpret the slope coefficient from the model you chose in 2.

```{r}
"Intercept: when is nitric oxides doesnt exist, the overall mortality rate is 6.81%.
log(nox): For each 1% difference in nitric oxides, the predicted difference 
in mortality rate is 0.02%."
```

4. Construct 99% confidence interval for slope coefficient from the model you chose in 2 and interpret them.

```{r}
confint(a2,level=0.99)
"we are 99% confident that the intercept lies between -4.742 and -4.669
we are 99% confident that the coefficient of log(nox) lies between 0.0017 and 0.0300
"
```

5. Now fit a model predicting mortality rate using levels of nitric oxides, sulfur dioxide, and hydrocarbons as inputs. Use appropriate transformations when
helpful. Plot the fitted regression model and interpret the coefficients.

```{r}
# check IQR 
apply(pollution[, c("hc", "nox", "so2")], FUN=IQR, MARGIN = 2)

# scale predictors
s2 <- function(X) (X - mean(X)) / (2*sd(X))
pollution[, c("hc_new", "nox_new", "so2_new")] <- apply(pollution[, c("hc", "nox", "so2")], FUN=s2, MARGIN = 2)

apply(pollution[, c("hc_new", "nox_new", "so2_new")], FUN=IQR, MARGIN = 2)

a3 <- lm(log(mort) ~ hc_new + nox_new + so2_new, data=pollution)

a3

par(mfrow=c(2,2))

plot(a3)

"from the residual plot output, we can say that the model  fits well. 

Intercept: The mortality rate for an individual exposed to average levels of nitric oxides,
sulfur dioxide, and hydrocarbons is exp(-39.2076)

hc_new : when one unit increase in hydrocarbons, the mortality rate would decrease by 27% 
(becasue it's $exp(-0.32) = 0.726$ times lower)

nox_new: when one unit nitric oxides increases, the mortality rate would be $exp(0.30) = 1.35$ 
times higher, which is 35% more.

so2_new: one unit difference for sulfur dioxide corresponds to 0.03% increase in mortality rate."
 
```

6. Cross-validate: fit the model you chose above to the first half of the data and then predict for the second half. (You used all the data to construct the model in 4, so this is not really cross-validation, but it gives a sense of how the steps of cross-validation can be implemented.)

```{r}

first_half <- pollution[1:30, ]
sec_half<- pollution[31:60, ]

a4 <- lm(log(mort) ~ hc_new + nox_new + so2_new, data=pollution)

display(a4)

predictions <- predict(a4, sec_half)

cbind(predictions=exp(predictions), observed=sec_half$mort)

```

### Study of teenage gambling in Britain

```{r,message =FALSE}
data(teengamb)
?teengamb
teengamb
```

1. Fit a linear regression model with gamble as the response and the other variables as predictors and interpret the coefficients. Make sure you rename and transform the variables to improve the interpretability of your regression model.

```{r}

gamble_log<-log(teengamb$gamble+1)
sex<-teengamb$sex

# center the status data
status_new<-(teengamb$status-mean(teengamb$status))/sd(teengamb$status)
income<-teengamb$income
verbal<-teengamb$verbal
m1<-lm(gamble_log~sex+status_new+income+verbal)

display(m1)


"from the model output, the r-squared is 0.52 which means that the model is okay in general. 
intercept: a male teenager with  Socioeconomic status score, no income and 0 verbal score spend exp(3.07) 
pounds per year for gambling.

sex: when Socioeconomic status score is 0, female teenager with no income and 0 verbal score spend exp(-0.87) 
pounds on gambling less than male on the same characteristic.

status_new : one unit increase in Socioeconomic status score, the overall spend on gambling for male
teen increase exp(0.51) pounds per year.

income: when one unit increase in income, male teenager with 0  Socioeconomic status score, no income 
and 0 verbal score tends to spend exp(0.22) pounds more on gambling per year.

verbal: one unit increase in verbal score increase correspond to the expenditure on gambling decrease 
exp(0.26) pounds per year for a male teen with no  Socioeconomic status score and no income "


```

2. Create a 95% confidence interval for each of the estimated coefficients and discuss how you would interpret this uncertainty.

```{r}
confint(m1,level=0.95)

"we are 95% confident that the intercept lies between 1.568 and 4.5629
we are 95% confident that the coefficint of sex lies between -1.66 to -0.07
we are 95% confident that the coefficient of Socioeconomic status score lies between 0.04 to 0.983
we are 95% confident that the coefficient of income lies between 0.1166 to 0.3146
we are 95% confident that the coefficient of verbal score lies between -0.47 to -0.052 "
```

3. Predict the amount that a male with average status, income and verbal score would gamble along with an appropriate 95% CI.  Repeat the prediction for a male with maximal values of status, income and verbal score.  Which CI is wider and why is this result expected?

```{r}
# p1 stands for the prediction of average score
p1<-predict(m1,newdata = data.frame(sex=0,status_new=0,income=mean(teengamb$income),verbal=mean(teengamb$verbal)),level = 0.95,interval  = "prediction")
p1
length1<-p1[3]-p1[2]
length1

#p2 stands for the prediction of max scores
p2<-predict(m1,newdata = data.frame(sex=0,status_new=max(teengamb$status)-mean(teengamb$status),income=max(teengamb$income),verbal=max(teengamb$verbal)),level = 0.95,interval  = "prediction")
p2
length2<-p2[3]-p2[2]
length2

# the width for a male with max scores is larger than the width of a male with average scores, which
#stands for the standard error of male with max scores is greater

```

### School expenditure and test scores from USA in 1994-95

```{r}
data(sat)
?sat
sat
```

1. Fit a model with total sat score as the outcome and expend, ratio and salary as predictors.  Make necessary transformation in order to improve the interpretability of the model.  Interpret each of the coefficient.

```{r}
# center ratio data

ratio_center<-sat$ratio-mean(sat$ratio)/sd(sat$ratio)
salary_new<-log(sat$salary)

model1<-lm(log(total)~ratio_center+log(salary),data=sat)
model1

display(model1)
plot(model1,which=1)

plot(predict(model1,type = "response"),residuals(model1,type = "deviance"))

"When ratio increase 1 unit, the total score will be exp(0.003) times of the original overall score

When log(salary) increase 1 unit, the total score will be exp(-0.212) times of the original overall score"


```

2. Construct 98% CI for each coefficient and discuss what you see.

```{r}
confint(model1,level=0.98)

"we are 98% confident that the intercept lies between 7.0654 and 8.114
we are 98% confident that the coefficient of ratio(after center) lies between -0.007 and 0.013
we are 98% confident that the coefficient of log(salary) lies between -0.357 and -0.066
"
```

3. Now add takers to the model.  Compare the fitted model to the previous model and discuss which of the model seem to explain the outcome better?

```{r}
model2<-lm(log(total)~ratio_center+log(salary)+log(sat$taker),data=sat)
display(model2)

 "the r-squared is 0.89 which means that 89% of the realtionsip between the dependent variables
can be explained by the model, the r-squared of the new model is greater than the previous model (0.21),
means that this model is much better than the previous one."
```

# Conceptual exercises.

### Special-purpose transformations:

For a study of congressional elections, you would like a measure of the relative amount of money raised by each of the two major-party candidates in each district. Suppose that you know the amount of money raised by each candidate; label these dollar values $D_i$ and $R_i$. You would like to combine these into a single variable that can be included as an input variable into a model predicting vote share for the Democrats.

Discuss the advantages and disadvantages of the following measures:

* The simple difference, $D_i-R_i$
advantage : it shows the difference between the two parties found and know how much democrats less/ more than the republicans.
disadvantage : it only shows the difference, the base of the two parties is unknown (for example, the difference between 2millions and 1million is the same as the difference between 4 millions and 3 millions)

* The ratio, $D_i/R_i$

advantages : it shows how much multiple the two parties differ.
disadvantages : the same as the previous question, it only shows the ratio, but not the nunber base.

* The difference on the logarithmic scale, $log D_i-log R_i$ 


* The relative proportion, $D_i/(D_i+R_i)$.
it shows the relative proportion of the two parties when compare together. however, it fails to show individual's advantages and disadvantages compare to the other competitor.

### Transformation 

For observed pair of $\mathrm{x}$ and $\mathrm{y}$, we fit a simple regression model 
$$\mathrm{y}=\alpha + \beta \mathrm{x} + \mathrm{\epsilon}$$ 
which results in estimates $\hat{\alpha}=1$, $\hat{\beta}=0.9$, $SE(\hat{\beta})=0.03$, $\hat{\sigma}=2$ and $r=0.3$.

1. Suppose that the explanatory variable values in a regression are transformed according to the $\mathrm{x}^{\star}=\mathrm{x}-10$ and that $\mathrm{y}$ is regressed on $\mathrm{x}^{\star}$.  Without redoing the regression calculation in detail, find $\hat{\alpha}^{\star}$, $\hat{\beta}^{\star}$, $\hat{\sigma}^{\star}$, and $r^{\star}$.  What happens to these quantities when $\mathrm{x}^{\star}=10\mathrm{x}$ ? When $\mathrm{x}^{\star}=10(\mathrm{x}-1)$?

$x^{\star}=x-10$, $\hat{\alpha}^{\star}=\hat{\alpha}+10\hat{\beta}=10$, $\hat{\beta}^{\star}=\hat{\beta}=0.9$, $r^{\star}=r=0.3$ ,$\hat{\sigma}^{\star}=\hat{\sigma}=2$.

$x^{\star}=10x$, $\hat{\alpha}^{\star}=\hat{\alpha}=1$, $\hat{\beta}^{\star}=\frac{\hat{\beta}}{10}=0.09$, $r^{\star}=r=0.3$ ,$\hat{\sigma}^{\star}=\hat{\sigma}=2$.

$x^{\star}=10(x-1)$, $\hat{\alpha}^{\star}=\hat{\alpha}+\hat{\beta}=1.9$, $\hat{\beta}^{\star}=\frac{\hat{\beta}}{10}=0.09$, $r^{\star}=r=0.3$ ,$\hat{\sigma}^{\star}=\hat{\sigma}=2$.


2. Now suppose that the response variable scores are transformed according to the formula
$\mathrm{y}^{\star\star}= \mathrm{y}+10$ and that $\mathrm{y}^{\star\star}$ is regressed on $\mathrm{x}$.  Without redoing the regression calculation in detail, find $\hat{\alpha}^{\star\star}$, $\hat{\beta}^{\star\star}$, $\hat{\sigma}^{\star\star}$, and $r^{\star\star}$.  What happens to these quantities when $\mathrm{y}^{\star\star}=5\mathrm{y}$ ? When $\mathrm{y}^{\star\star}=5(\mathrm{y}+2)$?

(1)$y^{\star\star}= y+10$, $\hat{\alpha}^{\star\star}=\hat{\alpha}+10=11$,

$\hat{\beta}^{\star\star}=\hat{\beta}=0.9$, $r^{\star}=r=0.3$

$\hat{\sigma}^{\star\star}=\hat{\sigma}=2$.

(2)$y^{\star\star}=5y$, $\hat{\alpha}^{\star\star}=5\hat{\alpha}=5$,

$\hat{\beta}^{\star\star}=5\hat{\beta}=4.5$, $r^{\star}=r=0.3$, $\hat{\sigma}^{\star\star}=5\hat{\sigma}=10$.

(3)$y^{\star\star}=5(y+2)$, $\hat{\alpha}^{\star\star}=5(\hat{\alpha}+2)=15$,

$\hat{\beta}^{\star\star}=5\hat{\beta}=4.5$, $r^{\star}=r=0.3$, $\hat{\sigma}^{\star\star}=5\hat{\sigma}=10$.

3. In general, how are the results of a simple regression analysis affected by linear transformations of $\mathrm{y}$ and $\mathrm{x}$?

when x plus or minus a constant number, it only changes the intercept. when changes the scale of x, it only change the slope coefficients. 

4. Suppose that the explanatory variable values in a regression are transformed according to the $\mathrm{x}^{\star}=10(\mathrm{x}-1)$ and that $\mathrm{y}$ is regressed on $\mathrm{x}^{\star}$.  Without redoing the regression calculation in detail, find $SE(\hat{\beta}^{\star})$ and $t^{\star}_0= \hat{\beta}^{\star}/SE(\hat{\beta}^{\star})$.

$SE(\hat{\beta}^{\star})=SE(\hat{\beta})=0.03$ 
$t^{\star}_0= \hat{\beta}^{\star}/SE(\hat{\beta}^{\star})=0.09/0.03=3$.

5. Now suppose that the response variable scores are transformed according to the formula
$\mathrm{y}^{\star\star}=5(\mathrm{y}+2)$ and that $\mathrm{y}^{\star\star}$ is regressed on $\mathrm{x}$.  Without redoing the regression calculation in detail, find $SE(\hat{\beta}^{\star\star})$ and $t^{\star\star}_0= \hat{\beta}^{\star\star}/SE(\hat{\beta}^{\star\star})$.

$SE(\hat{\beta}^{\star\star})=5*SE(\hat{\beta})=0.15$ and $t^{\star\star}_0= \hat{\beta}^{\star\star}/SE(\hat{\beta}^{\star\star})=30$

6. In general, how are the hypothesis tests and confidence intervals for $\beta$ affected by linear transformations of $\mathrm{y}$ and $\mathrm{x}$?

(a) $\frac{\bar{\beta}-\mu_0}{SE(\beta)}$~$t(n-1)$

Confidence Interval is $[\bar{\beta}-t_{\alpha/2}*SE(\beta),\bar{\beta}+t_{\alpha/2}*SE(\beta)]$

 if $x^=cx$, then $\bar{\beta^*}=\bar{\beta}/c$, 
 CI is $[\bar{\beta}/c-t_{\alpha/2}*SE(\beta)/c,\bar{\beta}/c+t_{\alpha/2}*SE(\beta)/c]$

If $y^=dy$, then $\bar{\beta^*}=\bar{\beta}*d$, 
CI is $[\bar{\beta}*d-t_{\alpha/2}*SE(\beta)*d,\bar{\beta}*d+t_{\alpha/2}*SE(\beta)*d]$

(b) In hypothesis test, $H_0$:$\mu=0$, $H_1$:$\mu\neq0$

$T=\frac{\bar{\beta}}{SE(\beta)}$~$t(n-1)$

And if $x^=cx$, then $\bar{\beta^*}=\bar{\beta}/c$,$T$.

If $y^=dy$, then $\bar{\beta^*}=\bar{\beta}*d$,$T$


		
# Feedback comments etc.

If you have any comments about the homework, or the class, please write your feedback here.  We love to hear your opinions.

